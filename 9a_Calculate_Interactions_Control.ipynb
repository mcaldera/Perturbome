{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcualte Control Perturbation Interactions  \n",
    "Perfore calculating the actual interaction one need to consider (same as 9_Calculate_Interactions):  \n",
    "1.) Check Drug Decay  (some drugs show symptoms of decay e.g. oxidiation --> less potency over time)  \n",
    "2.) Check CellCount  (if ther cellcount is below a certain number not statistical significant assumption can be made regarding the cell morphology)  \n",
    "\n",
    "To calculate meaningful interaction one need to check if the single and combination perturbation are significantly different from DMSO random fluctuation  \n",
    "3.) Check significance of single drugs  \n",
    "4.) Check significance of combinations  \n",
    "\n",
    "Finally random 'drug interaction' can be calculated  \n",
    "5.) Calculate the actual interaction  (take randomly two singles and a random combiantion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "from scipy.spatial import distance as dis\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from matplotlib import pylab as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drugs that have at least one correct well: 245\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extract the perturbation vectors as well as DMSO vectors (split per batch to reduce per batch effects)\n",
    "a.) drug_perturbation_vectors (drug perturbations)\n",
    "b.) dmso (DMSO)\n",
    "'''\n",
    "\n",
    "\n",
    "# Load all the drug perturbation vectors\n",
    "path = '../data/Calculate_Interactions/All_Vectors_Combined.csv'\n",
    "fp = open(path)\n",
    "\n",
    "features = fp.readline().split(',')[1:]\n",
    "numfeatures = len(features)\n",
    "\n",
    "drug_perturbation_vectors = {'Batch1':{},'Batch2':{}}\n",
    "dmso = {'Batch1':[],'Batch2':[]}\n",
    "\n",
    "All_CLOUDs = set()\n",
    "# Go threw the file and create DMSO and drug_perturbation_vectors\n",
    "# DMSO per batch\n",
    "# drug_perturbation_vectors per Batch with full identifier\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    \n",
    "    drug1, drug2 = tmp[0].split('_')[0].split('|')\n",
    "    well = tmp[0].split('_')[1]\n",
    "    plate = tmp[0].split('_')[2]\n",
    "    \n",
    "    values = list(np.float_(tmp[1:]))\n",
    "    \n",
    "    if int(plate) < 1315065:\n",
    "        if drug1 == 'DMSO':\n",
    "            dmso['Batch1'].append(values)\n",
    "        drug_perturbation_vectors['Batch1'][drug1+','+drug2+','+plate+','+well] = values\n",
    "            \n",
    "    else:\n",
    "        if drug1 == 'DMSO':\n",
    "            dmso['Batch2'].append(values)\n",
    "        drug_perturbation_vectors['Batch2'][drug1+','+drug2+','+plate+','+well] = values\n",
    "        \n",
    "        All_CLOUDs.add(drug1)\n",
    "\n",
    "#Get list of All_CLOUDs, this number is smaller than the original number due to some drugs not being transformer correctly or other problems\n",
    "All_CLOUDs.remove('DMSO')\n",
    "All_CLOUDs.remove('PosCon')\n",
    "\n",
    "All_CLOUDs = list(All_CLOUDs)\n",
    "All_CLOUDs.sort()\n",
    "print 'Number of drugs that have at least one correct well: %d' %len(All_CLOUDs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Drug Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drugs that decayed in batch1: 6\n",
      "Number of drugs that decayed in batch2: 2\n"
     ]
    }
   ],
   "source": [
    "# Both thresholds need to be true to set a drug as decayed during experiment; threshold_decay is steepness and threshold_MaxDifference absolute difference\n",
    "threshold_decay = 0.05\n",
    "threshold_MaxDifference = 0.3\n",
    "\n",
    "\n",
    "# Load all the drug decay regressions\n",
    "# Created by checking the single drug responses over the different plates (there is a temporal context between plate 1 and 123)\n",
    "# One is interested both in the decay as well as the maximum change e.g. if gradient between 0.1 to 0.2, still ok\n",
    "# Create a dic that tells about the status of drug decay i.e. True if drug WORKED CORRECTLY\n",
    "path = '../data/Calculate_Interactions/DrugDecay_Combined.csv'\n",
    "fp = open(path)\n",
    "fp.next()\n",
    "drug_decay = {}\n",
    "batch1_Failed = 0\n",
    "batch2_Failed = 0\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    \n",
    "    batch1_decay = float(tmp[1])\n",
    "    batch1_diff = float(tmp[2])\n",
    "    \n",
    "    batch2_decay = float(tmp[3])\n",
    "    batch2_diff = float(tmp[4])\n",
    "    \n",
    "    \n",
    "    batch1_Status = True\n",
    "    if batch1_decay >= threshold_decay and batch1_diff >= threshold_MaxDifference:\n",
    "        batch1_Status = False\n",
    "        batch1_Failed += 1\n",
    "        \n",
    "    batch2_Status = True\n",
    "    if batch2_decay >= threshold_decay and batch2_diff >= threshold_MaxDifference:\n",
    "        batch2_Status = False\n",
    "        batch2_Failed += 1\n",
    "    \n",
    "    \n",
    "    drug_decay[tmp[0]] = {'Batch1':batch1_Status,'Batch2':batch2_Status}\n",
    "fp.close()\n",
    "\n",
    "print 'Number of drugs that decayed in batch1: %d' %batch1_Failed\n",
    "print 'Number of drugs that decayed in batch2: %d' %batch2_Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cell Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of cells in Batch1: 630.938980\n",
      "Mean number of cells in Batch1: 1717.818782\n",
      "Number of empty wells in Batch1: 1433\n",
      "Number of empty wells in Batch2: 2221\n"
     ]
    }
   ],
   "source": [
    "#Minimum cells allowed\n",
    "cutoff_min_cells = 30\n",
    "\n",
    "#per well count\n",
    "well_cell_count = {}\n",
    "\n",
    "#number of wells below cutoff_min_cells\n",
    "empty_well = 0\n",
    "\n",
    "#go through CellCount file to find wells with too little cells\n",
    "path = '../data/Calculate_Interactions/All_CellCounts_Combined.csv'\n",
    "fp = open(path)\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    num_cells = float(tmp[4])\n",
    "    well_cell_count[tmp[0]+','+tmp[1]+','+tmp[2]+','+tmp[3]] = {'Number':num_cells,'Worked':tmp[5]}\n",
    "    \n",
    "#Mean number of cells (DMSO and Perturbations)\n",
    "print 'Mean number of cells in Batch1: %f'  %np.mean([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) < 1315065])\n",
    "print 'Mean number of cells in Batch1: %f'  %np.mean([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) >= 1315065])\n",
    "\n",
    "#Number of empty wells (DMSO and Perturbations)\n",
    "print 'Number of empty wells in Batch1: %d'  %len([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) < 1315065 and well_cell_count[x]['Number'] < cutoff_min_cells])\n",
    "print 'Number of empty wells in Batch2: %d'  %len([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) >= 1315065 and well_cell_count[x]['Number'] < cutoff_min_cells])\n",
    "\n",
    "#Get the 90 percentile of DMSO cellcount\n",
    "DMSO_CellCount = {}\n",
    "DMSO_CellCount['Batch1']= np.percentile([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) < 1315065 and x.split(',')[0] == 'DMSO'],90)\n",
    "DMSO_CellCount['Batch2']= np.percentile([well_cell_count[x]['Number'] for x in well_cell_count if int(x.split(',')[2]) >= 1315065 and x.split(',')[0] == 'DMSO'],90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which single Drugs are significantly morphologically changed\n",
    "Compared to DMSO random cell intrinsic morphological fluctuation (compare via Mahalanobis distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Random_Distribution_DMSO(DMSO_Wells,num_pseudo_treatments=4):\n",
    "    #Fit a PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(DMSO_Wells)\n",
    "\n",
    "    #Take only as many principel components so taht 90% of the variance can be explained\n",
    "    variances = pca.explained_variance_ratio_\n",
    "    total = 0\n",
    "    use_components = 0\n",
    "    for v in variances:\n",
    "        total = total + v\n",
    "        use_components += 1\n",
    "        if total > 0.9:\n",
    "            break\n",
    "\n",
    "    #Use at least 2 components\n",
    "    if use_components <= 1:\n",
    "        use_components = 2\n",
    "    \n",
    "    #Fit the space to the selected amount of components and transform the data\n",
    "    pca = PCA(n_components=use_components)\n",
    "    pca.fit(DMSO_Wells)\n",
    "    transformedX = pca.transform(DMSO_Wells)\n",
    "    weightedPCA = np.multiply(transformedX, pca.explained_variance_ratio_)\n",
    "    \n",
    "    all_mahala_distances = []\n",
    "    for i in range(0, 10000):\n",
    "        np.random.shuffle(weightedPCA)\n",
    "\n",
    "        treatments = weightedPCA[0:num_pseudo_treatments]\n",
    "        control = weightedPCA[num_pseudo_treatments:]\n",
    "\n",
    "        x = []\n",
    "        for i in range(0, use_components):\n",
    "            x.append(np.mean(treatments[:, i]))\n",
    "\n",
    "        u = []\n",
    "        for i in range(0, use_components):\n",
    "            u.append(np.mean(control[:, i]))\n",
    "\n",
    "        covariance_treatment = np.cov(treatments, rowvar=0)\n",
    "        covariance_control = np.cov(control, rowvar=0)\n",
    "\n",
    "        weighted_covariance_treatment = covariance_treatment * (float(len(treatments)) / (len(treatments) + len(control)))\n",
    "        weighted_covariance_control = covariance_control * (float(len(control)) / (len(treatments) + len(control)))\n",
    "\n",
    "        S = weighted_covariance_treatment + weighted_covariance_control\n",
    "\n",
    "        S = np.cov(weightedPCA, rowvar=0)\n",
    "\n",
    "        x = np.array(x)\n",
    "        u = np.array(u)\n",
    "        distance_rand = mahalanobis(x, u, np.linalg.inv(S))\n",
    "        all_mahala_distances.append(distance_rand)\n",
    "    \n",
    "    \n",
    "    return all_mahala_distances\n",
    "    \n",
    "if os.path.isfile('../results/Calculate_Interactions/RandomMahalanobisDistances_DMSO.pickle'):\n",
    "    pickle_in = open('../results/Calculate_Interactions/RandomMahalanobisDistances_DMSO.pickle',\"rb\")\n",
    "    Mahalanobis_Random_Distribution = pickle.load(pickle_in)\n",
    "else:\n",
    "    #Create Random Mahalanobis distributions for k = 1, 3,4,5,6,7 (1 = combination, 3-7 singles)\n",
    "    Mahalanobis_Random_Distribution = {'Batch1':{},'Batch2':{}}\n",
    "    for i in range(3,8):\n",
    "        Mahalanobis_Random_Distribution['Batch1'][i] = make_Random_Distribution_DMSO(dmso['Batch1'],i)\n",
    "    Mahalanobis_Random_Distribution['Batch1'][1] = make_Random_Distribution_DMSO(dmso['Batch1'],1)\n",
    "    print 'Finished Batch1 Random Distribution'\n",
    "    for i in range(3,8):\n",
    "        Mahalanobis_Random_Distribution['Batch2'][i] = make_Random_Distribution_DMSO(dmso['Batch2'],i)\n",
    "    Mahalanobis_Random_Distribution['Batch2'][1] = make_Random_Distribution_DMSO(dmso['Batch2'],1)\n",
    "    print 'Finished Batch2 Random Distribution'\n",
    "\n",
    "    pickle_out = open('../results/Calculate_Interactions/RandomMahalanobisDistances_DMSO.pickle',\"wb\")\n",
    "    pickle.dump(Mahalanobis_Random_Distribution, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_MahalanobisDistance(k,numberTreatment,number_of_RandomTries=10000, usePreComputedRandomDistribution = True, batch = None):\n",
    "    '''\n",
    "    This function takes a list of feature vectors, sorted with the first n (numberTreatment) belong to group A and the afterwards to group B.\n",
    "    1.) First the dimension of the feature vectors is reduced using PCA\n",
    "    2.) Only the m PCA dimensions are used so taht at least 90% of all variance is explained\n",
    "    3.) The indivudal dimensions are weighted by their importance\n",
    "    4.) Create the Mean Vector for the two grups (mean over columns)\n",
    "    5.) Calculate the Mahalanobis distance\n",
    "    6.) Calculate an empirical PValue by randomly permuting the labels of the groups and creating a random expected mahalanobis distance distribution; compare real mahalanobis distance\n",
    "        to this randomly drawn distribution to calulate a p value (basically: NumberOfRandomDrawn MahalanobisDistance larger than real / number of random Mahalanobis Distances)\n",
    "    '''\n",
    "    #Fit a PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(k)\n",
    "\n",
    "    #Take only as many principel components so taht 90% of the variance can be explained\n",
    "    variances = pca.explained_variance_ratio_\n",
    "    total = 0\n",
    "    use_components = 0\n",
    "    for v in variances:\n",
    "        total = total + v\n",
    "        use_components += 1\n",
    "        if total > 0.9:\n",
    "            break\n",
    "\n",
    "    #Use at least 2 components\n",
    "    if use_components <= 1:\n",
    "        use_components = 2\n",
    "    \n",
    "    #Fit the space to the selected amount of components and transform the data\n",
    "    pca = PCA(n_components=use_components)\n",
    "    pca.fit(k)\n",
    "    transformedX = pca.transform(k)\n",
    "\n",
    "    #weight the dimensions regarding the amount of variance explained\n",
    "    weightedPCA = np.multiply(transformedX, pca.explained_variance_ratio_)\n",
    "    pca1 = list(weightedPCA[:, 0])\n",
    "    pca2 = list(weightedPCA[:, 1])\n",
    "    \n",
    "    #split the list into the two groups (treated/untreated)\n",
    "    treatments = weightedPCA[0:numberTreatment]\n",
    "    control = weightedPCA[numberTreatment:]\n",
    "    \n",
    "    \n",
    "    #Contains the main treatment vector (Calculate mean vector - Over Columns)\n",
    "    x = []\n",
    "    for i in range(0, use_components):\n",
    "        x.append(np.mean(treatments[:, i]))\n",
    "\n",
    "    #Contains the mean untreated vector (Calculate mean vector - Over Columns)\n",
    "    u = []\n",
    "    for i in range(0, use_components):\n",
    "        u.append(np.mean(control[:, i]))\n",
    "\n",
    "    #Create the covariance matrix, as well as the groupA (treatment) mean vector and\n",
    "    # groupB (untreated) mean vector to finally calucate the mahalanobis distance between this two groups\n",
    "    S = np.cov(weightedPCA, rowvar=0)\n",
    "    x = np.array(x)\n",
    "    u = np.array(u)\n",
    "\n",
    "    \n",
    "    #Calculate the Mahalanobis distance between treated and untreated\n",
    "    distance_calc = mahalanobis(x, u, np.linalg.inv(S))\n",
    "\n",
    "    #original = weightedPCA.copy()\n",
    "    treatment_row = weightedPCA[0:numberTreatment].copy()\n",
    "\n",
    "    \n",
    "    if usePreComputedRandomDistribution == True:\n",
    "        all_mahala_distances = Mahalanobis_Random_Distribution[batch][numberTreatment]\n",
    "    else:\n",
    "        all_mahala_distances = []\n",
    "        for i in range(0, number_of_RandomTries):\n",
    "            np.random.shuffle(weightedPCA)\n",
    "\n",
    "\n",
    "            #if np.array_equal(original, weightedPCA):\n",
    "            if np.array_equal(treatment_row,weightedPCA[0:numberTreatment]):\n",
    "                continue\n",
    "\n",
    "            treatments = weightedPCA[0:numberTreatment]\n",
    "            control = weightedPCA[numberTreatment:]\n",
    "\n",
    "            x = []\n",
    "            for i in range(0, use_components):\n",
    "                x.append(np.mean(treatments[:, i]))\n",
    "\n",
    "            u = []\n",
    "            for i in range(0, use_components):\n",
    "                u.append(np.mean(control[:, i]))\n",
    "\n",
    "            covariance_treatment = np.cov(treatments, rowvar=0)\n",
    "            covariance_control = np.cov(control, rowvar=0)\n",
    "\n",
    "            weighted_covariance_treatment = covariance_treatment * (float(len(treatments)) / (len(treatments) + len(control)))\n",
    "            weighted_covariance_control = covariance_control * (float(len(control)) / (len(treatments) + len(control)))\n",
    "\n",
    "            S = weighted_covariance_treatment + weighted_covariance_control\n",
    "\n",
    "            S = np.cov(weightedPCA, rowvar=0)\n",
    "\n",
    "            x = np.array(x)\n",
    "            u = np.array(u)\n",
    "            distance_rand = mahalanobis(x, u, np.linalg.inv(S))\n",
    "            all_mahala_distances.append(distance_rand)\n",
    "\n",
    "\n",
    "        #print len([x for x in all_mahala_distances if x >= distance_calc]) / float(len(all_mahala_distances))\n",
    "\n",
    "    #caluclate empirical pvalue\n",
    "    mp = len([x for x in all_mahala_distances if x >= distance_calc]) / float(len(all_mahala_distances))        \n",
    "    if mp > 1:\n",
    "        mp = 1\n",
    "\n",
    "    \n",
    "    return distance_calc, mp, pca1, pca2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Easy Outlier detection\n",
    "def reject_outliers_2(data, m=6.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d / (mdev if mdev else 1.)\n",
    "    return s < m\n",
    "    #return [data[i] for i in range(0, len(data)) if s[i] < m]\n",
    "\n",
    "    \n",
    "\n",
    "def Create_Single_Drug_Vectors():\n",
    "    '''\n",
    "    Extracts the individual drug perturbation vectors (for the two batches separate).\n",
    "    Outliers e.g. one replicate that is too different from the remaining 5/6 will be removed.\n",
    "    \n",
    "    Only calculated for significant singles\n",
    "    '''\n",
    "    \n",
    "    drug_vectors = {'Batch1':{},'Batch2':{}}\n",
    "    for cloud in All_CLOUDs:\n",
    "        for b in ['Batch1','Batch2']:    \n",
    "            #if Single_Drug_Significance[cloud][b]['MP_Value'] < 0.01:\n",
    "\n",
    "            #print cloud\n",
    "            #change this func\n",
    "            drug_wells = [x for x in drug_perturbation_vectors[b] if cloud+',DMSO' in x and well_cell_count[x]['Number'] > cutoff_min_cells]\n",
    "\n",
    "            if len(drug_wells) < 3:\n",
    "                drug_vectors[b][cloud] = {}\n",
    "            else:\n",
    "                drug_well_values = []\n",
    "                for well in drug_wells:\n",
    "                    drug_well_values.append(drug_perturbation_vectors[b][well])\n",
    "\n",
    "                drug_well_values = np.array(drug_well_values)\n",
    "\n",
    "                drug_EucledianDistances = []\n",
    "                drug_names = []\n",
    "                drug_values = []\n",
    "                for s1_a,label_a in zip(drug_well_values,drug_wells):\n",
    "                    tmp = []\n",
    "                    for s1_b,label_b in zip(drug_well_values,drug_wells):\n",
    "                        sim = np.linalg.norm(s1_a - s1_b) \n",
    "                        tmp.append(sim)\n",
    "\n",
    "                    drug_EucledianDistances.append(np.mean(tmp))\n",
    "                    drug_names.append(label_a)\n",
    "                    drug_values.append(s1_a)\n",
    "\n",
    "\n",
    "                good_rows = reject_outliers_2(drug_EucledianDistances)\n",
    "                keep_values = [drug_values[x] for x in range(0,len(drug_values)) if good_rows[x]]\n",
    "                keep_names = [drug_names[x] for x in range(0,len(drug_names)) if good_rows[x]]\n",
    "\n",
    "                drug_vectors[b][cloud] = {}\n",
    "                for val, lab in zip(keep_values,keep_names):\n",
    "                    drug_vectors[b][cloud][lab] = val\n",
    "\n",
    "    return drug_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary for all single drugs, and remove obvious outliers\n",
    "Outliers are detected by first calculating the mean eucledian distance of a point to all other points and then performing an MAD outlier detection\n",
    "'''\n",
    "drug_vectors_WithoutOutliers = Create_Single_Drug_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant drugs in Batch1: 45\n",
      "Number of significant drugs in Batch2: 31\n"
     ]
    }
   ],
   "source": [
    "Single_Drug_Significance = {}\n",
    "\n",
    "#If this file already exists it doesn't need to be calculated again (if you wish to recalculate it you need to delete it first)\n",
    "if os.path.isfile('../results/Calculate_Interactions/Singles/Overview.csv'):\n",
    "    fp = open('../results/Calculate_Interactions/Singles/Overview.csv','r')\n",
    "    fp.next()\n",
    "    for line in fp:\n",
    "        tmp = line.strip().split(',')\n",
    "        Single_Drug_Significance[tmp[0]] = {'Batch1':{},'Batch2':{}}\n",
    "        \n",
    "        if tmp[1] == 'No_Cells':\n",
    "            Single_Drug_Significance[tmp[0]]['Batch1']['Mahalanobis_Distance'] = 'Nan'\n",
    "            Single_Drug_Significance[tmp[0]]['Batch1']['MP_Value'] = 'Nan'\n",
    "        else:\n",
    "            Single_Drug_Significance[tmp[0]]['Batch1']['Mahalanobis_Distance'] = float(tmp[1])\n",
    "            Single_Drug_Significance[tmp[0]]['Batch1']['MP_Value'] = float(tmp[2])\n",
    "            \n",
    "        if tmp[3] == 'No_Cells':\n",
    "            Single_Drug_Significance[tmp[0]]['Batch2']['Mahalanobis_Distance'] = 'Nan'\n",
    "            Single_Drug_Significance[tmp[0]]['Batch2']['MP_Value'] = 'Nan'\n",
    "        else:\n",
    "            Single_Drug_Significance[tmp[0]]['Batch2']['Mahalanobis_Distance'] = float(tmp[3])\n",
    "            Single_Drug_Significance[tmp[0]]['Batch2']['MP_Value'] = float(tmp[4])\n",
    "            \n",
    "else:\n",
    "    #Go through all clouds\n",
    "    for cloud in All_CLOUDs:\n",
    "        print cloud\n",
    "        #cloud = 'CLOUD031'\n",
    "        Single_Drug_Significance[cloud] = {}\n",
    "        #calculate the significance for both batches separately\n",
    "        for b in ['Batch1','Batch2']:\n",
    "\n",
    "            #Create a list with Treatments (drug vectors) and DMSO; Including only those perturbations that have enough cells\n",
    "            \n",
    "            #Get the single perturbations\n",
    "            k = drug_vectors_WithoutOutliers[b][cloud].values()\n",
    "            numberTreatment = len(k)\n",
    "            \n",
    "            #Only perform an analysis if at least 3 replicates exist\n",
    "            if numberTreatment < 3:\n",
    "                Single_Drug_Significance[cloud][b] = {'Mahalanobis_Distance':'No_Cells','MP_Value':'No_Cells'}\n",
    "            \n",
    "            else:\n",
    "                #if enough replicates, add the DMSO wells to the list\n",
    "                k.extend(dmso[b])\n",
    "\n",
    "                #Calculate the mahalanobis distance as well as empirical pValue.\n",
    "                distance_calc, mp, pca1, pca2 = Calculate_MahalanobisDistance(k,numberTreatment,True,b)\n",
    "\n",
    "                \n",
    "                '''\n",
    "                PLOT RESULTS\n",
    "                '''\n",
    "                if mp < 0.01:\n",
    "                    color = ['#b2182b']*numberTreatment\n",
    "                else:\n",
    "                     color = ['#2166ac']*numberTreatment\n",
    "\n",
    "                for i in range(1,len(k)):\n",
    "                    color.append('grey')\n",
    "\n",
    "                Single_Drug_Significance[cloud][b] = {'Mahalanobis_Distance':distance_calc,'MP_Value':mp}\n",
    "\n",
    "                plt.scatter(pca1[numberTreatment:], pca2[numberTreatment:], c=color[numberTreatment:], alpha=0.4)\n",
    "                plt.scatter(pca1[0:numberTreatment], pca2[0:numberTreatment], c=color[0:numberTreatment], alpha=0.4)\n",
    "                plt.legend(['DMSO','Mahalanobis Distance: %.2f\\n mp = %.2e\\n n = %d' % (distance_calc, mp,numberTreatment)])\n",
    "                #plt.show()\n",
    "                plt.savefig('../results/Calculate_Interactions/Singles/'+cloud+'_'+b+'.png')\n",
    "                plt.close()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    WRITE OUTPUT\n",
    "    '''\n",
    "    fp_out = open('../results/Calculate_Interactions/Singles/Overview.csv','w')\n",
    "    fp_out.write('Drug,Batch1_Mahalanobis_Distance,Batch1_MP_Value,Batch2_Mahalanobis_Distance,Batch2_MP_Value\\n')\n",
    "    for cloud in All_CLOUDs:\n",
    "        fp_out.write(cloud+','+str(Single_Drug_Significance[cloud]['Batch1']['Mahalanobis_Distance'])+','+str(Single_Drug_Significance[cloud]['Batch1']['MP_Value'])+','+str(Single_Drug_Significance[cloud]['Batch2']['Mahalanobis_Distance'])+','+str(Single_Drug_Significance[cloud]['Batch2']['MP_Value'])+'\\n')\n",
    "    fp_out.close()\n",
    "\n",
    "\n",
    "#print Single_Drug_Significance\n",
    "\n",
    "print 'Number of significant drugs in Batch1: %d' %len([x for x in Single_Drug_Significance if Single_Drug_Significance[x]['Batch1']['MP_Value'] < 0.01])\n",
    "print 'Number of significant drugs in Batch2: %d' %len([x for x in Single_Drug_Significance if Single_Drug_Significance[x]['Batch2']['MP_Value'] < 0.01])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which problems should be removed from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drugs to remove:\n",
      "{'Batch2': ['CLOUD080', 'CLOUD180', 'CLOUD197'], 'Batch1': ['CLOUD065', 'CLOUD083', 'CLOUD120', 'CLOUD139', 'CLOUD205', 'CLOUD243']}\n",
      "Cytotoxic drugs/non usable morphological features:\n",
      "{'Batch2': ['CLOUD024', 'CLOUD152', 'CLOUD193'], 'Batch1': ['CLOUD024', 'CLOUD179', 'CLOUD193']}\n"
     ]
    }
   ],
   "source": [
    "#list of drugs that kill too many cells to do morphological analysis with them (Remove only for Morphology Analysis)\n",
    "killing_drugs = {'Batch1':[],'Batch2':[]}\n",
    "\n",
    "#this drugs must be removed from further analysis (Both the CellCount and Morphology Analysis)\n",
    "drugs_to_remove = {'Batch1':[],'Batch2':[]}\n",
    "\n",
    "for cloud in All_CLOUDs:\n",
    "    for b in ['Batch1','Batch2']:\n",
    "        \n",
    "        #Check if enough replicates exist\n",
    "        enough_replicates_status = len(drug_vectors_WithoutOutliers[b][cloud]) >= 3\n",
    "        \n",
    "        #Get result from drug decay analayis\n",
    "        drug_decay_status = drug_decay[cloud][b]\n",
    "        \n",
    "        #Check if both are good\n",
    "        if enough_replicates_status and drug_decay_status:\n",
    "            continue\n",
    "        else:\n",
    "            #print 'Bad Drug (%s): %s    Decay?: %s' %(b,cloud, str(not drug_decay_status))\n",
    "            \n",
    "            #if the drug does not decay and has enough working wells not image problems, you can use it still for CellCount Anlaysis (again if at least 3 wells)\n",
    "            if drug_decay_status:\n",
    "                number_replicates = len([x for x in drug_perturbation_vectors[b] if cloud+',DMSO' in x])\n",
    "                        \n",
    "                if number_replicates >= 3:\n",
    "                    #print '==> can be used a \"killing drug\"'\n",
    "                    killing_drugs[b].append(cloud)\n",
    "                else:\n",
    "                    drugs_to_remove[b].append(cloud)\n",
    "            else:\n",
    "                drugs_to_remove[b].append(cloud)\n",
    "\n",
    "print 'Drugs to remove:'\n",
    "print drugs_to_remove\n",
    "\n",
    "print 'Cytotoxic drugs/non usable morphological features:'\n",
    "print killing_drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of DMSO wells after a very strong (MAD=10) outlier detection; these will serve as random intracellular fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmso_wells_Outlier_Removed = {'Batch1':{},'Batch2':{}}\n",
    "\n",
    "#Go through both batches\n",
    "for b in ['Batch1','Batch2']:\n",
    "    #get all plates of this batch\n",
    "    plates = list(set([x.split(',')[2] for x in drug_perturbation_vectors[b].keys()]))\n",
    "    #go through all plates\n",
    "    for plate in plates:\n",
    "        \n",
    "        #Get corresponding DMSO wells of this plate\n",
    "        DMSO_Wells  = [drug_perturbation_vectors[b][x] for x in drug_perturbation_vectors[b] if 'DMSO,None,'+str(plate) in x and well_cell_count[x]['Number'] > cutoff_min_cells]\n",
    "        DMSO_Wells = np.array(DMSO_Wells)\n",
    "        \n",
    "        #Perform outlier detection\n",
    "        DMSO_EucledianDistances = []\n",
    "        DMSO_values = []\n",
    "        for s1_a in DMSO_Wells:\n",
    "            tmp = []\n",
    "            for s1_b in DMSO_Wells:\n",
    "                sim = np.linalg.norm(s1_a - s1_b) \n",
    "                tmp.append(sim)\n",
    "\n",
    "            DMSO_EucledianDistances.append(np.mean(tmp))\n",
    "            DMSO_values.append(s1_a)\n",
    "        \n",
    "        #Perform actual MAD outlier detection with MAD = 10\n",
    "        good_rows = reject_outliers_2(DMSO_EucledianDistances,2)\n",
    "        keep_values = [DMSO_values[x] for x in range(0,len(DMSO_values)) if good_rows[x]]\n",
    "        \n",
    "        #Add result\n",
    "        dmso_wells_Outlier_Removed[b][plate] = keep_values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual Math for calculating the DDIs\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "def calculate_vector_math_v2(a, b, c):\n",
    "    '''\n",
    "    calculate the amount of single a, single b, and the 'surprise factor)\n",
    "    :param a: vector a (single)\n",
    "    :param b: vector b (single)\n",
    "    :param c: vector c (combination)\n",
    "    :return: alpha, beta and gamma (part of vector a, b and suprise)\n",
    "    '''\n",
    "\n",
    "    if sum(a) != 0 and sum(b) != 0:\n",
    "\n",
    "        \n",
    "        if angle_between(a,b) <= 0.5:\n",
    "            #h = c/(a+b)\n",
    "\n",
    "            A=  np.array([[np.dot(a, a), np.dot(a, b)], [np.dot(a, b), np.dot(b, b)]])\n",
    "            h = np.array([np.dot(a, c), np.dot(b, c)])\n",
    "\n",
    "            alpha =  h[0]/(A[0][0]+A[1][0])\n",
    "            beta = h[1] / (A[0][1]+A[1][1])\n",
    "\n",
    "            n =alpha * a + beta * b - c\n",
    "\n",
    "            gamma = np.linalg.norm(n)\n",
    "            return str(alpha),str(beta),str(gamma)\n",
    "\n",
    "        elif angle_between(a,b) == 3.141592653589793:\n",
    "            A = np.array([[np.dot(a, a), np.dot(a, b)], [np.dot(a, b), np.dot(b, b)]])\n",
    "            h = np.array([np.dot(a, c), np.dot(b, c)])\n",
    "\n",
    "\n",
    "            alpha = h[0] / (A[0][0] + abs(A[1][0]))\n",
    "            beta = h[1] / (abs(A[0][1]) + A[1][1])\n",
    "\n",
    "            n = alpha * a + beta * b - c\n",
    "\n",
    "            gamma = np.linalg.norm(n)\n",
    "\n",
    "            return str(alpha), str(beta), str(gamma)\n",
    "\n",
    "    try:\n",
    "\n",
    "        \n",
    "        if sum(c) != 0 and sum(a) == 0 and sum(b) == 0:\n",
    "            return '1','1',str(dis.euclidean([0]*len(c),c))\n",
    "        elif sum(c) == 0 and sum(a) == 0 and sum(b) == 0:\n",
    "            return '1','1','0'\n",
    "        elif sum(c) == 0 and sum(a) == 0:\n",
    "            return '1','0','0'\n",
    "        elif sum(c) == 0 and sum(b) == 0:\n",
    "            return '0','1','0'\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Matrix equation\n",
    "            A = np.array([[np.dot(a, a), np.dot(a, b)], [np.dot(a, b), np.dot(b, b)]])\n",
    "\n",
    "            h = np.array([np.dot(a, c), np.dot(b, c)])\n",
    "\n",
    "            \n",
    "            if A[0][0]==0 and h[0] ==0: #one vector zero, so combination can be only 1dim\n",
    "\n",
    "                beta = h[1]/A[1][1]\n",
    "                n = beta * b -c\n",
    "                gamma = np.linalg.norm(n)\n",
    "\n",
    "                if  (len(list(b)) - list(b).count(0) > 2) or np.linalg.norm(b) > 0.5:\n",
    "                    return '1.0',str(beta),str(gamma)\n",
    "                else:\n",
    "                    return '1', '1', str(dis.euclidean([0] * len(c), c))\n",
    "            elif A[1][1]==0 and h[1] ==0:\n",
    "                alpha = h[0]/A[0][0]\n",
    "                n = alpha * a -c\n",
    "                gamma = np.linalg.norm(n)\n",
    "\n",
    "\n",
    "                if len(list(a)) - list(a).count(0) > 2 or np.linalg.norm(a) > 0.5:\n",
    "                    return str(alpha),'1',str(gamma)\n",
    "                else:\n",
    "                    return '1', '1', str(dis.euclidean([0] * len(c), c))\n",
    "            elif h[0] == 0 and h[1] == 0:\n",
    "                gamma = np.linalg.norm(c)\n",
    "                return '0.0','0.0',str(gamma)\n",
    "            elif A[0][0] != 0 and h[0] == 0 and h[1] == 0:\n",
    "                gamma = np.linalg.norm(c)\n",
    "                return '0.0','1.0',str(gamma)\n",
    "            elif A[1][1] != 0 and h[0] == 0 and h[1] == 0:\n",
    "                gamma = np.linalg.norm(c)\n",
    "                return '1.0','0',str(gamma)\n",
    "\n",
    "            p = np.linalg.solve(A, h)\n",
    "            # orthogonal vector\n",
    "            n = p[0] * a + p[1] * b - c\n",
    "\n",
    "            distance = np.linalg.norm(n)\n",
    "            # check\n",
    "            # print('dot product of a and c: %.4f' %(np.dot(a,n)))\n",
    "            # print('dot product of b and c: %.4f' %(np.dot(b,n)))\n",
    "            # print('distance: %.3f' %(distance))\n",
    "            return str(p[0]), str(p[1]), str(distance)\n",
    "    except:\n",
    "        return 'Error', 'Error', 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLOUD181,CLOUD143,1315033,F06']\n",
      "[763.0, 523.0, 750.0, 938.0, 857.0]\n",
      "[888.0, 914.0, 786.0, 790.0, 807.0, 869.0, 798.0]\n",
      "CLOUD101,CLOUD072,Batch1,1315033,F06,2.59808543649,0.1554,0.2079002285776088,3.75426968303,0.0156,0.3438197249593796,17.1165313188492,0.0053,1.6855590943287004,-3.0126685594220355,0.8203752592172106,1.58785713618704,1,1,1.68555909433,8.869117581747519,0.0\n",
      "\n",
      "['CLOUD075,CLOUD256,1315057,H09']\n",
      "[934.0, 1054.0, 908.0, 858.0, 900.0, 910.0, 969.0]\n",
      "[664.0, 431.0, 573.0, 634.0, 447.0, 507.0]\n",
      "CLOUD101,CLOUD072,Batch1,1315057,H09,2.59808543649,0.1554,0.2079002285776088,3.75426968303,0.0156,0.3438197249593796,5.519479260359544,0.3457,0.42891795015386414,1,1,0,1,1,0,0,1\n",
      "\n",
      "['CLOUD045,CLOUD202,1315094,F22']\n",
      "[2022.0, 2269.0, 2298.0, 2114.0, 2147.0, 2110.0]\n",
      "[2208.0, 2452.0, 2232.0, 2200.0, 2006.0]\n",
      "CLOUD101,CLOUD072,Batch2,1315094,F22,7.33627102884,0.0001,0.4368796246880667,5.83348648014,0.0141,0.34871105472662806,4.884373596252041,0.1513,0.24580541370870476,-0.04171879578522473,0.4969039615753594,0.1808832703014244,0,1,0,7.616092567557578,0.0\n",
      "\n",
      "['CLOUD061,CLOUD122,1315075,B23']\n",
      "[2685.0, 2662.0, 2332.0, 2229.0, 2820.0, 2186.0]\n",
      "[1817.0, 2459.0, 2274.0, 2622.0, 2041.0, 2238.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-fae4ddec7f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcombi_Vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m#Here 500 random tries is already enough since only ca. 250 datapoints in 'possible' results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mcombination_distance_calc_toNI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombination_mp_toNI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculate_MahalanobisDistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberTreatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_of_RandomTries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musePreComputedRandomDistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8e944a814891>\u001b[0m in \u001b[0;36mCalculate_MahalanobisDistance\u001b[0;34m(k, numberTreatment, number_of_RandomTries, usePreComputedRandomDistribution, batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mall_mahala_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_RandomTries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightedPCA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fp_out= open('../results/Calculate_Interactions_Random/MC_Scores/'+str(my_Number)+'.csv','w')\n",
    "fp_out.write('SingleDrug1,SingleDrug2,Combination,Batch,Plate,Well,S1_Mahalanobis,S1_MPValue,S1_Norm,S2_Mahalanobis,S2_MPValue,S2_Norm,Combi_Mahalanobis,Combi_MPValue,Combi_Norm_Norm,Alpha,Beta,Gamma,Alpha_Zero,Beta_Zero,Gamma_Zero,Mahalanobis_Distance_To_NI,MPValue_To_NI\\n')\n",
    "\n",
    "\n",
    "\n",
    "#Number_Combinations = len(All_CLOUDs) * (len(All_CLOUDs) -1) / 2\n",
    "\n",
    "for i in range(0,250):\n",
    "    print i\n",
    "    #Choose random clouds\n",
    "    combination_cloud1, combination_cloud2 =  random.sample(All_CLOUDs,2)\n",
    "    single_cloud1,single_cloud2 = random.sample(All_CLOUDs,2)\n",
    "    \n",
    "    \n",
    "    if (combination_cloud1 == combination_cloud2) or (combination_cloud1 == single_cloud1) or (combination_cloud1 == single_cloud2) or (combination_cloud2 == single_cloud1) or (combination_cloud2 == single_cloud2) or (single_cloud1 == single_cloud2):\n",
    "        print 'Not allowed shuffeling'\n",
    "        continue\n",
    "    \n",
    "    #Name of the combination\n",
    "    combination_Pair = combination_cloud1+';'+combination_cloud2\n",
    "    \n",
    "    #Get the random combination well\n",
    "    combination_well =  [x for x in well_cell_count if combination_cloud1+','+combination_cloud2 in x or combination_cloud2+','+combination_cloud1 in x]\n",
    "    \n",
    "    #print combination_well\n",
    "    #This should never happen\n",
    "    if len(combination_well) == 0:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    #Extract the plate and well information\n",
    "    plate = combination_well[0].split(',')[2]\n",
    "    well = combination_well[0].split(',')[3]\n",
    "\n",
    "    #Extract the batch information from the plate number\n",
    "    if int(plate) < 1315065:\n",
    "        b = 'Batch1'\n",
    "    else:\n",
    "        b= 'Batch2'\n",
    "\n",
    "    #Check if either for the two drugs are in the drugs_to_remove list ==> all this interactions must be discarded (e.g. because the drug oxidized over course of screen)\n",
    "    if single_cloud1 in drugs_to_remove[b]:\n",
    "        fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['Drug1Problem']*17)+'\\n')\n",
    "        continue\n",
    "    if single_cloud2 in drugs_to_remove[b]:\n",
    "        fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['Drug2Problem']*17)+'\\n')\n",
    "        continue\n",
    "\n",
    "    #Get the single replicates\n",
    "    single1_replicate_wells = [x for x in drug_perturbation_vectors[b] if single_cloud1+',DMSO' in x]\n",
    "    single2_replicate_wells = [x for x in drug_perturbation_vectors[b] if single_cloud2+',DMSO' in x]\n",
    "\n",
    "    #Get the Combination well cell count, and the transfer status\n",
    "    combi_Number =  well_cell_count[combination_well[0]]['Number']\n",
    "    combi_Worked =  well_cell_count[combination_well[0]]['Worked']\n",
    "\n",
    "    #Get the singles cell counts\n",
    "    s1_cellCount = [well_cell_count[x]['Number'] for x in single1_replicate_wells if well_cell_count[x]['Worked'] == 'TRUE']\n",
    "    s2_cellCount = [well_cell_count[x]['Number'] for x in single2_replicate_wells if well_cell_count[x]['Worked'] == 'TRUE']\n",
    "\n",
    "\n",
    "    #print s1_cellCount\n",
    "    #print s2_cellCount\n",
    "    \n",
    "    #If there was a problem with the drug transfer ===> all this interactions must be discarded (i.e. because no drug was transfered in this well)\n",
    "    if combi_Worked == 'FALSE':\n",
    "        #print 'This combination had a transfer problem' \n",
    "        fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['CombinationTransferProblem']*17)+'\\n')\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "\n",
    "\n",
    "        #Check if one of the singles is a cytotoxic drug (i.e. has too few cells in the well) or the combination is cytotoxic\n",
    "        if single_cloud1 in killing_drugs[b]:\n",
    "            fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['CytotoxicDrug1']*17)+'\\n')\n",
    "            continue\n",
    "        if single_cloud2 in killing_drugs[b]:\n",
    "            fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['CytotoxicDrug2']*17)+'\\n')\n",
    "            continue\n",
    "        if combi_Number < cutoff_min_cells:\n",
    "            fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['CytotoxicCombination']*17)+'\\n')\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        MORPHOLOGY - MC SCORE CALCULATION\n",
    "        '''\n",
    "\n",
    "\n",
    "        #################\n",
    "        # FROM HERE actual morphology\n",
    "        # 1.) check if the two singles and/or combination are significant morphological changers\n",
    "        # 2.) depending on that calculate interactions\n",
    "        ###########\n",
    "\n",
    "\n",
    "        #Check if combination feature vector exist (if not there might have been a problem with the well; e.g. filtered for bad quality)\n",
    "        if drug_perturbation_vectors[b].has_key(combination_well[0]) == False:\n",
    "            fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+','.join(['ProblemWithCombinationWell']*17)+'\\n')\n",
    "            continue\n",
    "\n",
    "        #Get the combination vector\n",
    "        combi_Vector =  drug_perturbation_vectors[b][combination_well[0]]\n",
    "\n",
    "\n",
    "        k = [combi_Vector]\n",
    "        numberTreatment = len(k)\n",
    "        k.extend(dmso[b])\n",
    "\n",
    "\n",
    "        #Calculate the mahalanobis distance as well as empirical pValue.\n",
    "        combination_distance_calc, combination_mp, pca1, pca2 = Calculate_MahalanobisDistance(k,numberTreatment,usePreComputedRandomDistribution=True,batch=b)\n",
    "\n",
    "        #Calculate the vector length (=norm)\n",
    "        Combi_VectorNorm = np.linalg.norm(combi_Vector)\n",
    "\n",
    "        #Get the significance of drug1 and drug2\n",
    "        s1_Mahalanobis = Single_Drug_Significance[single_cloud1][b]['Mahalanobis_Distance'] \n",
    "        s1_MValue = Single_Drug_Significance[single_cloud1][b]['MP_Value'] \n",
    "        s2_Mahalanobis = Single_Drug_Significance[single_cloud2][b]['Mahalanobis_Distance'] \n",
    "        s2_MValue = Single_Drug_Significance[single_cloud2][b]['MP_Value'] \n",
    "\n",
    "        \n",
    "\n",
    "        ########\n",
    "        # SINGLE 1\n",
    "        #check if drug1 is significant (single1)\n",
    "        s1_significant = False\n",
    "        if s1_MValue < 0.01  and s1_Mahalanobis > 6:\n",
    "            s1_significant = True\n",
    "        s1_Vectors = drug_vectors_WithoutOutliers[b][single_cloud1].values()     \n",
    "\n",
    "        #Create a numpy array\n",
    "        val =  np.array(s1_Vectors)\n",
    "\n",
    "        #Transform into mean vector\n",
    "        s1_MeanVector = []\n",
    "        for i in range(0,numfeatures):\n",
    "            s1_MeanVector.append(np.mean(val[:,i]))\n",
    "\n",
    "        #get s1 vector length (=norm)\n",
    "        s1_Mean_VectorNorm = np.linalg.norm(s1_MeanVector)\n",
    "\n",
    "\n",
    "\n",
    "        ########\n",
    "        # SINGLE 2\n",
    "        #check if drug2 is significant (single2)\n",
    "        s2_significant = False\n",
    "        if s2_MValue < 0.01  and s2_Mahalanobis > 6:\n",
    "            s2_significant = True\n",
    "        s2_Vectors = drug_vectors_WithoutOutliers[b][single_cloud2].values()    \n",
    "\n",
    "        #Create a numpy array\n",
    "        val =  np.array(s2_Vectors)\n",
    "\n",
    "        #Transform into mean vector\n",
    "        s2_MeanVector = []\n",
    "        for i in range(0,numfeatures):\n",
    "            s2_MeanVector.append(np.mean(val[:,i]))\n",
    "\n",
    "        #get s2 vector length (=norm)\n",
    "        s2_Mean_VectorNorm = np.linalg.norm(s2_MeanVector)\n",
    "\n",
    "        ########\n",
    "        # Combination\n",
    "        #check if combination is significant; no need to create a mean vector as only one replicate\n",
    "        comb_significant = False\n",
    "        if combination_mp < 0.05  and combination_distance_calc > 6:\n",
    "            comb_significant = True\n",
    "\n",
    "        #Only if at least one of the three players is significantly perturbed\n",
    "        #if no_interaction == False: \n",
    "\n",
    "        #Transform the three lists into numpy arrays\n",
    "        s1_MeanVector_toCalculate = np.array(s1_MeanVector)\n",
    "        s2_MeanVector_toCalculate = np.array(s2_MeanVector)\n",
    "        comb_MeanVector_toCalculate = np.array(combi_Vector)              \n",
    "\n",
    "        #Calculate alpha/beta/gamma\n",
    "        alpha,beta,gamma = calculate_vector_math_v2(s1_MeanVector_toCalculate,s2_MeanVector_toCalculate,comb_MeanVector_toCalculate)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Additionally one can also set non significant vector to the NULL vectors (then the math is forced to only use significant ones)\n",
    "        '''\n",
    "\n",
    "        if s1_significant == False:\n",
    "            s1_MeanVector_toCalculate = np.array([0]*numfeatures)                                \n",
    "        if s2_significant == False:\n",
    "            s2_MeanVector_toCalculate = np.array([0]*numfeatures)                      \n",
    "        if comb_significant == False:\n",
    "            comb_MeanVector_toCalculate = np.array([0]*numfeatures)\n",
    "\n",
    "        #Calculate alpha_0/beta_0/gamma_0\n",
    "        alpha_0,beta_0,gamma_0 = calculate_vector_math_v2(s1_MeanVector_toCalculate,s2_MeanVector_toCalculate,comb_MeanVector_toCalculate)\n",
    "\n",
    "\n",
    "        #Get DMSO wells (of this plate) after outlier detection (these serve ONLY as an ADDITIONAL cell fluctation that is added to all NI (vector sums))\n",
    "        DMSO_Wells = dmso_wells_Outlier_Removed[b][plate]\n",
    "\n",
    "        #Calculate all possible NIs and add the DMSO cell fluctuation to them (randomly choose 10 DMSO)\n",
    "        possible_results = []\n",
    "        for s1 in s1_Vectors:\n",
    "            tmp = []\n",
    "            for s2 in s2_Vectors:\n",
    "                vector_sum = np.array(s1) + np.array(s2)\n",
    "                possible_results.append(vector_sum)\n",
    "                #random_DMSO_Wells = random.sample(DMSO_Wells,25)\n",
    "                for DMSO_well in DMSO_Wells:\n",
    "                    possible_results.append(vector_sum+np.array(DMSO_well))\n",
    "\n",
    "        k = list(possible_results)\n",
    "        k.insert(0,combi_Vector)\n",
    "        #Here 500 random tries is already enough since only ca. 250 datapoints in 'possible' results\n",
    "        combination_distance_calc_toNI, combination_mp_toNI, _, _ = Calculate_MahalanobisDistance(k,numberTreatment,number_of_RandomTries = 10000, usePreComputedRandomDistribution = False)\n",
    "\n",
    "\n",
    "           \n",
    "        fp_out.write(single_cloud1+','+single_cloud2+','+combination_Pair+','+str(b)+','+plate+','+well+','+str(s1_Mahalanobis)+','+str(s1_MValue)+','+str(s1_Mean_VectorNorm) +','+str(s2_Mahalanobis)+','+str(s2_MValue)+','+str(s2_Mean_VectorNorm)+','+str(combination_distance_calc)+','+str(combination_mp)+','+str(Combi_VectorNorm)+','+str(alpha)+','+str(beta)+','+str(gamma)+','+str(alpha_0)+','+str(beta_0)+','+str(gamma_0)+','+str(combination_distance_calc_toNI)+','+str(combination_mp_toNI) +'\\n')\n",
    "\n",
    "\n",
    "print 'Created all interactions'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Summary File\n",
      "MC_Scores:\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print 'Make Summary File'\n",
    "print 'MC_Scores:'\n",
    "MC_Score_Files = [f for f in os.listdir('../results/Calculate_Interactions_Random/MC_Scores/') if os.path.isfile(os.path.join('../results/Calculate_Interactions_Random/MC_Scores/', f))]\n",
    "MC_Score_Files.sort()\n",
    "fp_out = open('../results/Calculate_Interactions_Random/All_MC_Scores.csv','w')\n",
    "fp_out.write('SDrug1,SDrug2,CDrug1,CDrug2,Batch,Plate,Well,S1_Mahalanobis,S1_MPValue,S1_Norm,S2_Mahalanobis,S2_MPValue,S2_Norm,Combi_Mahalanobis,Combi_MPValue,Combi_Norm_Norm,Alpha,Beta,Gamma,Alpha_Zero,Beta_Zero,Gamma_Zero,Mahalanobis_Distance_To_NI,MPValue_To_NI\\n')\n",
    "for file_name in MC_Score_Files:\n",
    "    #print file_name\n",
    "    fp = open('../results/Calculate_Interactions_Random/MC_Scores/'+file_name,'r')\n",
    "    fp.next()\n",
    "    for line in fp:\n",
    "        fp_out.write(line)\n",
    "    fp.close()\n",
    "fp_out.close()\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
