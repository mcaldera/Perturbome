{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of drugs\n",
    "Find various properties of the individual drugs  \n",
    "  \n",
    "1.) ATC  \n",
    "2.) GO Annotations  \n",
    "3.) Disease   \n",
    "4.) KeGG Pathways  \n",
    "5.) SIDER (known effects)  \n",
    "6.) Offside (known off sides)  \n",
    "7.) Drug Properties (physico-chemical properties)  \n",
    "8.) Enzymes, Transporters and Carriers  \n",
    "9.) Chemical_Gentic Perturbations (MsigDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrugBank Network loaded\n",
      "Finished ATC annotations\n"
     ]
    }
   ],
   "source": [
    "import  networkx as nx\n",
    "\n",
    "#The the ATC classification from drugbank (see python file: 2a_Create_DrugBank_Network.ipynb)\n",
    "DrugBankInfo = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "print 'DrugBank Network loaded'\n",
    "\n",
    "#Create output file\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_ATC.csv','w')\n",
    "fp_out.write('CLOUD,DrugBankID,First_Level_ATCs,Second_Level_ATCs\\n')\n",
    "\n",
    "\n",
    "DrugBank_to_CLOUD = {}\n",
    "#parse through all CLOUD drugs and check for ATC code annotation in drugbank (Use first and second level; third level and below too specific)\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv','r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    DrugBank_to_CLOUD[tmp[1]] = tmp[0]\n",
    "    first_level = set()\n",
    "    fist_second_level = set()\n",
    "    if DrugBankInfo.has_node(tmp[1]):\n",
    "        if DrugBankInfo.node[tmp[1]].has_key('ATCcode'):\n",
    "            atc_codes =  DrugBankInfo.node[tmp[1]]['ATCcode'].split(',')\n",
    "            if '' in atc_codes:\n",
    "                atc_codes.remove('')\n",
    "\n",
    "            for atc in atc_codes:\n",
    "                atc = atc.strip()\n",
    "                first_level.add(atc[0])\n",
    "                fist_second_level.add(atc[0:3])\n",
    "\n",
    "    fp_out.write(tmp[0]+','+tmp[1]+','+';'.join(first_level)+','+';'.join(fist_second_level)+'\\n')\n",
    "\n",
    "fp.close()\n",
    "fp_out.close()\n",
    "\n",
    "print 'Finished ATC annotations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function\n",
      "Process\n",
      "Component\n",
      "Finished GO\n"
     ]
    }
   ],
   "source": [
    "#use our inhouse database and the corresponding python file to create the upward ontology for every leaf GO term (all get included)\n",
    "#Download (http://www.geneontology.org/page/downloads)\n",
    "import gene2terms_addupstream as GO\n",
    "\n",
    "#Include all threee GO branches\n",
    "go_branches = ['Function','Process','Component']\n",
    "\n",
    "#Find all the targets for the individual cloud drugs\n",
    "cloud_targets = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_All_Targets.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_targets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "\n",
    "all_clouds = cloud_targets.keys()\n",
    "all_clouds.sort()\n",
    "\n",
    "#Go throug the GO branches and find GO terms for a specific drug via: Drug --> Targets --> Associated GO-Terms\n",
    "drug_to_GO = {}\n",
    "for go_branch in go_branches:\n",
    "    print go_branch\n",
    "    drug_to_GO[go_branch] = {}\n",
    "    GO_Association_UP, GO_genes_annotation = GO.getAllGene_Annotation(go_branch)\n",
    "      \n",
    "    for drug in all_clouds:\n",
    "        drug_to_GO[go_branch][drug] = []\n",
    "        for target in cloud_targets[drug]:\n",
    "            drug_to_GO[go_branch][drug].extend(GO_Association_UP[target])\n",
    "        drug_to_GO[go_branch][drug] = list(set(drug_to_GO[go_branch][drug]))\n",
    "        \n",
    "#Save CLOUD drug to GO term annotations\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_GOterms.csv','w')\n",
    "fp_out.write('CLOUD,GO_Function,GO_Process,GO_Component\\n')\n",
    "for cloud in all_clouds:\n",
    "    fp_out.write(cloud+','+';'.join(drug_to_GO['Function'][cloud])+','+';'.join(drug_to_GO['Process'][cloud])+','+';'.join(drug_to_GO['Component'][cloud])+'\\n')\n",
    "fp_out.close()\n",
    "\n",
    "print 'Finished GO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-11-9c9250f0f3ed>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-9c9250f0f3ed>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# Download from http://www.disgenet.org/web/DisGeNET/menu/downloads and http://disease-ontology.org/downloads/\n",
    "# Again use inhouse database (manually curated), and corresponding scripts \n",
    "\n",
    "# Get all cloud drug targets\n",
    "fp = open('../data/Drug_Properties/CLOUD_All_Targets.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_targets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "all_clouds = cloud_targets.keys()\n",
    "all_clouds.sort()\n",
    "\n",
    "#Extrate the upward disease ontology (find all disease associated leaf plus upwards ontology terms for a specific gene)\n",
    "Disease_Association_UP,d_diseases_annotation = GO.getAllGene_Disease_Annotation()\n",
    "\n",
    "\n",
    "all_proteins = Disease_Association_UP.keys()\n",
    "all_proteins  = [int(x) for x in all_proteins]\n",
    "all_proteins.sort()\n",
    "\n",
    "fp_out = open('../results/Drug_Properties/Gene_to_Disease.csv','w')\n",
    "fp_out.write('Gene,Disease_ID\\n')\n",
    "for protein in all_proteins:\n",
    "    fp_out.write(str(protein)+','+';'.join(Disease_Association_UP[str(protein)])+'\\n')\n",
    "fp_out.close()\n",
    "\n",
    "\n",
    "\n",
    "break\n",
    "\n",
    "\n",
    "#associated drug with diseaes\n",
    "drug_to_Diseases = {}\n",
    "for drug in all_clouds:\n",
    "        drug_to_Diseases[drug] = []\n",
    "        for target in cloud_targets[drug]:\n",
    "            drug_to_Diseases[drug].extend(Disease_Association_UP[target])\n",
    "        drug_to_Diseases[drug] = list(set(drug_to_Diseases[drug]))\n",
    "        \n",
    "\n",
    "\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_Disease.csv','w')\n",
    "fp_out.write('CLOUD,Disease_ID\\n')\n",
    "for cloud in all_clouds:\n",
    "    fp_out.write(cloud+','+';'.join(drug_to_Diseases[cloud])+'\\n')\n",
    "fp_out.close()\n",
    "\n",
    "print 'Finished Diseases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeGG Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrugBank Network loaded\n",
      "Number of pathways directed targeted: 106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Get KeGG pathways via the biopython.KEGG REST \n",
    "from Bio.KEGG import REST\n",
    "\n",
    "#Find the KeGG identifiers via the drugbank annotations\n",
    "DrugBankInfo = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "print 'DrugBank Network loaded'\n",
    "\n",
    "#parse through all CLOUD targets\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv','r')\n",
    "fp.next()\n",
    "drug_to_pathways = {}\n",
    "all_targeted_Pathways = set()\n",
    "all_clouds = []\n",
    "kegg_IDs = {}\n",
    "\n",
    "#find the KeGG Drug page and find PATHWAY informations (direct drug to pathway)\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    \n",
    "    drug_to_pathways[tmp[0]] = []\n",
    "    \n",
    "    all_clouds.append(tmp[0])\n",
    "    \n",
    "    if DrugBankInfo.has_node(tmp[1]):\n",
    "        if DrugBankInfo.node[tmp[1]].has_key('KEGGDrug'):\n",
    "            kegg_ID = DrugBankInfo.node[tmp[1]]['KEGGDrug']\n",
    "            kegg_IDs[tmp[0]] = kegg_ID\n",
    "            drug_file = REST.kegg_get(kegg_ID).read()\n",
    "\n",
    "            for line in drug_file.rstrip().split(\"\\n\"):\n",
    "                section = line[:12].strip()  # section names are within 12 columns\n",
    "                if not section == \"\":\n",
    "                    current_section = section\n",
    "                if current_section == \"PATHWAY\":\n",
    "                    tmp2 =  line[12:].split('  ')\n",
    "                    pathwayID = tmp2[0].split('(')[0]\n",
    "                    drug_to_pathways[tmp[0]].append(pathwayID)\n",
    "                    all_targeted_Pathways.add(pathwayID)\n",
    "                    \n",
    "print 'Number of pathways directed targeted: %d' %len(all_targeted_Pathways)\n",
    "\n",
    "all_clouds.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "Number of pathways indirected targeted: 293\n",
      "Finished Pathways\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Additonally to finding the direct annotations, also find drug --> targets --> pathways associated to those target annotations\n",
    "'''\n",
    "\n",
    "#Get all targets\n",
    "cloud_targets = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_All_Targets.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_targets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "# find human pahtways\n",
    "human_pathways = REST.kegg_list(\"pathway\", \"hsa\").read()\n",
    "\n",
    "# get all human pathways, and add the dictionary\n",
    "pathways = {}\n",
    "for line in human_pathways.rstrip().split(\"\\n\"):\n",
    "    entry, description = line.split(\"\\t\")\n",
    "    pathways[entry] =  {'Description' :description, 'IDs':None,'Symbols':None}\n",
    "\n",
    "\n",
    "print len(pathways)\n",
    "# Get the genes for pathways and add them to a list\n",
    "\n",
    "for pathway in pathways.keys():\n",
    "    pathway_file = REST.kegg_get(pathway).read()  # query and read each pathway\n",
    "\n",
    "    # iterate through each KEGG pathway file, keeping track of which section\n",
    "    # of the file we're in, only read the gene in each pathway\n",
    "    current_section = None\n",
    "\n",
    "    genesSymbols =  []\n",
    "    genesIDs = []\n",
    "    for line in pathway_file.rstrip().split(\"\\n\"):\n",
    "        section = line[:12].strip()  # section names are within 12 columns\n",
    "        if not section == \"\":\n",
    "            current_section = section\n",
    "\n",
    "        if current_section == \"GENE\":\n",
    "            if ';' in line:\n",
    "                gene_identifiers, gene_description = line[12:].split(\"; \")\n",
    "                gene_id, gene_symbol = gene_identifiers.split()\n",
    "\n",
    "                if not gene_id in genesIDs:\n",
    "                    genesIDs.append(gene_id)\n",
    "                    genesSymbols.append(gene_symbol)\n",
    "\n",
    "    pathways[pathway] = genesIDs\n",
    "\n",
    "via_target_assigned_Pathways = {}\n",
    "second_assigned_pathways = set()\n",
    "for cloud in all_clouds:\n",
    "    via_target_assigned_Pathways[cloud] = [] \n",
    "    targets = cloud_targets[cloud]\n",
    "    for p in pathways:\n",
    "        if len(set(targets).intersection(set(pathways[p]))) > 0:\n",
    "            via_target_assigned_Pathways[cloud].append(p)\n",
    "            second_assigned_pathways.add(p)\n",
    "            \n",
    "print 'Number of pathways indirected targeted: %d' %len(second_assigned_pathways)\n",
    "\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_KeGG_Pathways.csv','w')\n",
    "fp_out.write('CLOUD,KeGG_DrugID,KeGG_Assigned_Pathways,Via_Target_Assigned\\n')\n",
    "for cloud in all_clouds:\n",
    "    if kegg_IDs.has_key(cloud):\n",
    "        fp_out.write(cloud+','+kegg_IDs[cloud]+','+';'.join(drug_to_pathways[cloud])+','+';'.join(via_target_assigned_Pathways[cloud])+'\\n')\n",
    "    else:\n",
    "        fp_out.write(cloud+',,'+';'.join(drug_to_pathways[cloud])+','+';'.join(via_target_assigned_Pathways[cloud])+'\\n')\n",
    "        \n",
    "fp_out.close()\n",
    "\n",
    "print 'Finished Pathways'        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATC_To_PubChem(isOffsides = 'None'):\n",
    "    '''\n",
    "    Sider offerst a direct conversion from ATC code to the internally used PubChem ID.\n",
    "    Offers a better coverage. \n",
    "    \n",
    "    Download: http://sideeffects.embl.de/download/ [Nov. 2018] drug_atc.tsv file\n",
    "    (here named: Pubchem_To_ATC)\n",
    "    '''\n",
    "\n",
    "    dic_ATc_To_Pubchem = {}\n",
    "    fp = open('../data/Drug_Properties/Pubchem_To_ATC.tsv')\n",
    "    for line in fp:\n",
    "        tmp =  line.strip().split('\\t')\n",
    "        dic_ATc_To_Pubchem[tmp[1]] = tmp[0]\n",
    "\n",
    "    cloud_drugs = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "    \n",
    "    \n",
    "    #find pubchem identifiers via ATC identifiers (as pubchem identifiers sometimes not unique neithers SID nor CID)\n",
    "    cloud_to_Pubchem = {}\n",
    "    PubChem_to_cloud = {}\n",
    "    found_PubChems = []\n",
    "    for drugBankID in cloud_drugs.nodes():\n",
    "        if cloud_drugs.node[drugBankID].has_key('ATCcode'):\n",
    "            all_codes = [x.strip() for x in cloud_drugs.node[drugBankID]['ATCcode'].split(',') if x != '']\n",
    "            for code in all_codes:\n",
    "                if dic_ATc_To_Pubchem.has_key(code):\n",
    "                    pubChemID = dic_ATc_To_Pubchem[code][3:]\n",
    "                    if isOffsides == 'offsides':\n",
    "                        tmp = list(pubChemID)\n",
    "                        tmp[0] = '0'\n",
    "                        pubChemID = ''.join(tmp)\n",
    "\n",
    "                    cloud_to_Pubchem[drugBankID] = pubChemID\n",
    "                    PubChem_to_cloud[pubChemID] = drugBankID\n",
    "                    found_PubChems.append(pubChemID)\n",
    "\n",
    "\n",
    "    return cloud_to_Pubchem, PubChem_to_cloud,found_PubChems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with SIDER\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download SIDER.tsv from http://sideeffects.embl.de/download/ [Nov. 2018] \n",
    "'''\n",
    "\n",
    "#get the different identifiers of a drug\n",
    "DrugBank_To_CLOUD = {}\n",
    "CLOUD_To_DrugBank = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv')\n",
    "fp.next()\n",
    "all_clouds = []\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    all_clouds.append(tmp[0])\n",
    "    DrugBank_To_CLOUD[tmp[1]] = tmp[0]\n",
    "    CLOUD_To_DrugBank[tmp[0]] = tmp[1]\n",
    "fp.close()\n",
    "\n",
    "all_clouds.sort()\n",
    "\n",
    "#extract pubchem identifier via ATC codes\n",
    "DrugBank_to_Pubchem_viaATC, PubChem_to_cloud_viaATC,found_PubChems_viaATC = ATC_To_PubChem()\n",
    "\n",
    "#further use drugbank to find additional pubchem identifiers for the cloud drugs\n",
    "cloud_drugs = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "\n",
    "#associate cloud with the different pubchem identifiers\n",
    "pubchemCompound_To_DrugBank = {}\n",
    "DrugBank_to_PubChem = {}\n",
    "pubchemCompound = []\n",
    "pubchemSubstance = []\n",
    "for node in cloud_drugs.nodes():\n",
    "    if cloud_drugs.node[node].has_key('PubChemCompound'):\n",
    "        pubchemCompound.append(cloud_drugs.node[node]['PubChemCompound'])\n",
    "        pubchemCompound_To_DrugBank[cloud_drugs.node[node]['PubChemCompound']] = node\n",
    "        DrugBank_to_PubChem[node] = cloud_drugs.node[node]['PubChemCompound']\n",
    "\n",
    "#Combine both dictionaries together\n",
    "for key in DrugBank_to_Pubchem_viaATC:\n",
    "    DrugBank_to_PubChem[key] = DrugBank_to_Pubchem_viaATC[key]\n",
    "\n",
    "\n",
    "\n",
    "#check the SIDER database for given sideeffect of a given drug (once via the ATC to pubchem identfiers; once via drugbank to pubchem)\n",
    "compund_sideEffect = {}\n",
    "fp = open('../data/Drug_Properties/SIDER.tsv','r')\n",
    "for line in fp:\n",
    "    tmp = line.strip().split('\\t')\n",
    "    id1 = tmp[1][3:]\n",
    "    id2 = tmp[2][3:]\n",
    "\n",
    "    if id1 in found_PubChems_viaATC:\n",
    "        if compund_sideEffect.has_key(PubChem_to_cloud_viaATC[id1]):\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id1]].append(tmp[3])\n",
    "        else:\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id1]] = [tmp[3]]\n",
    "\n",
    "    if id1 in pubchemCompound:\n",
    "        if compund_sideEffect.has_key(pubchemCompound_To_DrugBank[id1]):\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id1]].append(tmp[3])\n",
    "        else:\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id1]] = [tmp[3]]\n",
    "\n",
    "\n",
    "\n",
    "    if id2 in found_PubChems_viaATC:\n",
    "        if compund_sideEffect.has_key(PubChem_to_cloud_viaATC[id2]):\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id2]].append(tmp[3])\n",
    "        else:\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id2]] = [tmp[3]]\n",
    "\n",
    "    if id2 in pubchemCompound:\n",
    "        if compund_sideEffect.has_key(pubchemCompound_To_DrugBank[id2]):\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id2]].append(tmp[3])\n",
    "        else:\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id2]] = [tmp[3]]\n",
    "\n",
    "##\n",
    "# Save results\n",
    "##\n",
    "\n",
    "\n",
    "fp = open('../results/Drug_Properties/CLOUD_to_SIDER.csv','w')\n",
    "fp.write('CLOUD,PubChem,SIDER_Ids\\n')\n",
    "for key in all_clouds:\n",
    "    if compund_sideEffect.has_key(CLOUD_To_DrugBank[key]):\n",
    "        fp.write(key +','+DrugBank_to_PubChem[CLOUD_To_DrugBank[key]]+','+';'.join(list(set(compund_sideEffect[CLOUD_To_DrugBank[key]])))+'\\n')\n",
    "    elif DrugBank_to_PubChem.has_key(CLOUD_To_DrugBank[key]):\n",
    "        fp.write(key  +','+DrugBank_to_PubChem[CLOUD_To_DrugBank[key]]+',' + '\\n')\n",
    "    else:\n",
    "        fp.write(key + ',,\\n')\n",
    "fp.close()\n",
    "\n",
    "print 'Finish with SIDER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offsides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "Finish with OFFSIDES\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download Offsides.tsv from http://tatonettilab.org/resources/tatonetti-stm.html [Nov. 2018] \n",
    "'''\n",
    "#get the different identifiers of a drug\n",
    "DrugBank_To_CLOUD = {}\n",
    "CLOUD_To_DrugBank = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    DrugBank_To_CLOUD[tmp[1]] = tmp[0]\n",
    "    CLOUD_To_DrugBank[tmp[0]] = tmp[1]\n",
    "fp.close()\n",
    "\n",
    "#extract pubchem identifier via ATC codes\n",
    "DrugBank_to_Pubchem_viaATC, PubChem_to_cloud_viaATC, found_PubChems_viaATC = ATC_To_PubChem('offsides')\n",
    "\n",
    "#further use drugbank to find additional pubchem identifiers for the cloud drugs\n",
    "cloud_drugs = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "\n",
    "#associate cloud with the different pubchem identifiers\n",
    "pubchemCompound_To_DrugBank = {}\n",
    "DrugBank_to_PubChem = {}\n",
    "pubchemCompound = []\n",
    "pubchemSubstance = []\n",
    "for node in cloud_drugs.nodes():\n",
    "    if cloud_drugs.node[node].has_key('PubChemCompound'):\n",
    "        pubchemCompound.append(cloud_drugs.node[node]['PubChemCompound'].zfill(9))\n",
    "        pubchemCompound_To_DrugBank[cloud_drugs.node[node]['PubChemCompound'].zfill(9)] = node\n",
    "        DrugBank_to_PubChem[node] = cloud_drugs.node[node]['PubChemCompound'].zfill(9)\n",
    "\n",
    "# Combine both dictionaries together\n",
    "for key in DrugBank_to_Pubchem_viaATC:\n",
    "    DrugBank_to_PubChem[key] = DrugBank_to_Pubchem_viaATC[key]\n",
    "\n",
    "\n",
    "#check the OFFSIDES database for given sideeffect of a given drug (once via the ATC to pubchem identfiers; once via drugbank to pubchem)\n",
    "compund_sideEffect = {}\n",
    "fp = open('../data/Drug_Properties/Offsides.tsv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split('\\t')\n",
    "\n",
    "\n",
    "    id1 = tmp[0].replace('\"','')[3:]\n",
    "    sideEffect = tmp[2].replace('\"','')\n",
    "\n",
    "    #print id1\n",
    "\n",
    "    if id1 in found_PubChems_viaATC:\n",
    "        if compund_sideEffect.has_key(PubChem_to_cloud_viaATC[id1]):\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id1]].append(sideEffect)\n",
    "        else:\n",
    "            compund_sideEffect[PubChem_to_cloud_viaATC[id1]] = [sideEffect]\n",
    "            print len(compund_sideEffect.keys())\n",
    "            # print compund_sideEffect.keys()\n",
    "\n",
    "    if id1 in pubchemCompound:\n",
    "        if compund_sideEffect.has_key(pubchemCompound_To_DrugBank[id1]):\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id1]].append(sideEffect)\n",
    "        else:\n",
    "            compund_sideEffect[pubchemCompound_To_DrugBank[id1]] = [sideEffect]\n",
    "            print len(compund_sideEffect.keys())\n",
    "            # print compund_sideEffect.keys()\n",
    "\n",
    "fp = open('../results/Drug_Properties/CLOUD_to_Offsides.csv', 'w')\n",
    "fp.write('CLOUD,PubChem,OFFSIDE_Ids\\n')\n",
    "for key in all_clouds:\n",
    "    if compund_sideEffect.has_key(CLOUD_To_DrugBank[key]):\n",
    "        fp.write(key  +','+DrugBank_to_PubChem[CLOUD_To_DrugBank[key]]+','+';'.join(list(set(compund_sideEffect[CLOUD_To_DrugBank[key]])))+'\\n')\n",
    "    elif DrugBank_to_PubChem.has_key(CLOUD_To_DrugBank[key]):\n",
    "        fp.write(key + ',' +DrugBank_to_PubChem[CLOUD_To_DrugBank[key]]+',' + '\\n')\n",
    "    else:\n",
    "        fp.write(key + ',,\\n')\n",
    "fp.close()\n",
    "\n",
    "print 'Finish with OFFSIDES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoSides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with TwoSides\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download Offsides.tsv from http://tatonettilab.org/resources/tatonetti-stm.html [Nov. 2018] \n",
    "'''\n",
    "#get the different identifiers of a drug\n",
    "DrugBank_To_CLOUD = {}\n",
    "CLOUD_To_DrugBank = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    DrugBank_To_CLOUD[tmp[1]] = tmp[0]\n",
    "    CLOUD_To_DrugBank[tmp[0]] = tmp[1]\n",
    "fp.close()\n",
    "\n",
    "#extract pubchem identifier via ATC codes\n",
    "DrugBank_to_Pubchem_viaATC, PubChem_to_cloud_viaATC, found_PubChems_viaATC = ATC_To_PubChem('offsides')\n",
    "\n",
    "#further use drugbank to find additional pubchem identifiers for the cloud drugs\n",
    "cloud_drugs = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "\n",
    "pubchemCompound_To_DrugBank = {}\n",
    "DrugBank_to_PubChem = {}\n",
    "pubchemCompound = []\n",
    "pubchemSubstance = []\n",
    "for node in cloud_drugs.nodes():\n",
    "    if cloud_drugs.node[node].has_key('PubChemCompound'):\n",
    "        pubchemCompound.append(cloud_drugs.node[node]['PubChemCompound'].zfill(9))\n",
    "        pubchemCompound_To_DrugBank[cloud_drugs.node[node]['PubChemCompound'].zfill(9)] = node\n",
    "        DrugBank_to_PubChem[node] = cloud_drugs.node[node]['PubChemCompound'].zfill(9)\n",
    "\n",
    "# Combine both dictionaries together\n",
    "for key in DrugBank_to_Pubchem_viaATC:\n",
    "    DrugBank_to_PubChem[key] = DrugBank_to_Pubchem_viaATC[key]\n",
    "    \n",
    "    \n",
    "#check the SIDER database for given sideeffect of a given drug (once via the ATC to pubchem identfiers; once via drugbank to pubchem)\n",
    "TwoSide_Network = nx.Graph()\n",
    "fp = open('../data/Drug_Properties/TwoSides.tsv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split('\\t')\n",
    "\n",
    "\n",
    "    id1 = tmp[0][3:]\n",
    "    id2 = tmp[1][3:]\n",
    "    sideEffect = tmp[4]\n",
    "\n",
    "    #print id1\n",
    "    found_id1 = None\n",
    "    found_id2 = None\n",
    "    \n",
    "    if id1 in found_PubChems_viaATC: \n",
    "        found_id1 = PubChem_to_cloud_viaATC[id1]\n",
    "    elif id1 in pubchemCompound: \n",
    "        found_id1 = pubchemCompound_To_DrugBank[id1]\n",
    "        \n",
    "    if found_id1 != None:\n",
    "        if id2 in found_PubChems_viaATC: \n",
    "            found_id2 = PubChem_to_cloud_viaATC[id2]\n",
    "        elif id2 in pubchemCompound: \n",
    "            found_id2 = pubchemCompound_To_DrugBank[id2]\n",
    "        \n",
    "        \n",
    "        if found_id2 != None:\n",
    "            if TwoSide_Network.has_edge(found_id1,found_id2) == False:\n",
    "                TwoSide_Network.add_edge(found_id1,found_id2)\n",
    "                TwoSide_Network[found_id1][found_id2]['SideEffect'] = sideEffect\n",
    "            else:\n",
    "                 TwoSide_Network[found_id1][found_id2]['SideEffect'] =  TwoSide_Network[found_id1][found_id2]['SideEffect']  +',' + sideEffect\n",
    "        \n",
    "        \n",
    "nx.write_gml(TwoSide_Network,'../results/Drug_Properties/TwoSide_CLOUDs.gml')\n",
    "\n",
    "print 'Finish with TwoSides'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrugBank Network loaded\n",
      "Finish with Chemical Properties\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Physicochemical properties (calculated) offered by DrugBank\n",
    "'''\n",
    "\n",
    "#List of interesting physicochemical properties (continues)\n",
    "Continuesfeatures = ['Polarizability','logS','logP','NumberofRings','PhysiologicalCharge',\n",
    "            'PolarSurfaceAreaPSA','pKastrongestbasic','pKastrongestacidic',\n",
    "            'Refractivity','MonoisotopicWeight','HBondDonorCount',\n",
    "            'RotatableBondCount','WaterSolubility']\n",
    "\n",
    "##List of interesting physicochemical properties (discrete)\n",
    "discreteFeatures = ['DrugSubClass','DrugClass','Family']\n",
    "\n",
    "#Drugbank file\n",
    "DrugBankInfo = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03_CLOUD_Only.gml')\n",
    "print 'DrugBank Network loaded'\n",
    "\n",
    "#output file\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_PubChem_Chembl.csv','r')\n",
    "fp.next()\n",
    "\n",
    "\n",
    "#parse through all cloud drugs and find physicochemical propterties\n",
    "CLOUD_Chemical_properties = {}\n",
    "all_clouds = []\n",
    "kegg_IDs = {}\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    \n",
    "    all_clouds.append(tmp[0])\n",
    "    CLOUD_Chemical_properties[tmp[0]] = {}\n",
    "    \n",
    "    \n",
    "    if DrugBankInfo.has_node(tmp[1]):\n",
    "        CLOUD_Chemical_properties[tmp[0]]['DrugBankID'] = tmp[1]\n",
    "        \n",
    "        for c in Continuesfeatures:\n",
    "            if  DrugBankInfo.node[tmp[1]].has_key(c):\n",
    "                CLOUD_Chemical_properties[tmp[0]][c] = str(DrugBankInfo.node[tmp[1]][c])\n",
    "            else:\n",
    "                CLOUD_Chemical_properties[tmp[0]][c] = 'None'\n",
    "                \n",
    "        \n",
    "        for d in discreteFeatures:\n",
    "            if  DrugBankInfo.node[tmp[1]].has_key(d):\n",
    "                CLOUD_Chemical_properties[tmp[0]][d] = str(DrugBankInfo.node[tmp[1]][d])\n",
    "            else:\n",
    "                CLOUD_Chemical_properties[tmp[0]][d] = 'None'\n",
    "        \n",
    "    else:\n",
    "        CLOUD_Chemical_properties[tmp[0]]['DrugBankID'] = 'None'\n",
    "        \n",
    "        for c in Continuesfeatures:\n",
    "            CLOUD_Chemical_properties[tmp[0]][c] = 'None'\n",
    "            \n",
    "        for d in discreteFeatures:\n",
    "            CLOUD_Chemical_properties[tmp[0]][d] = 'None'\n",
    "            \n",
    "##\n",
    "# Save results\n",
    "##\n",
    "        \n",
    "fp = open('../results/Drug_Properties/CLOUD_to_ChemicalProperties.tsv', 'w')\n",
    "fp.write('CLOUD\\tDrugBankID\\t')\n",
    "fp.write('\\t'.join(Continuesfeatures)+'\\t'+'\\t'.join(discreteFeatures)+'\\n')        \n",
    "         \n",
    "for cloud in all_clouds:\n",
    "        fp.write(cloud+'\\t'+CLOUD_Chemical_properties[cloud]['DrugBankID'])\n",
    "        for c in Continuesfeatures:\n",
    "             fp.write('\\t'+CLOUD_Chemical_properties[cloud][c])\n",
    "        for d in discreteFeatures:\n",
    "             fp.write('\\t'+CLOUD_Chemical_properties[cloud][d])\n",
    "        fp.write('\\n')\n",
    "fp.close()\n",
    "\n",
    "print 'Finish with Chemical Properties'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets, Enzymes, Transporters and Carriers\n",
    "Split the full lust of targets into targets, enzymes, transporters and carriers\n",
    "Therefore use the DrugBank annotations of what a target, transporter, carrier and enzyme is. Go trough all drugbank targets and take the corresponding annotations.\n",
    "Then go trough the CLOUD targets and assign the targets accordingly. If drugbank does not show any annotation the gene is assumed to be a target.\n",
    "\n",
    "Enzymes: e.g. CYP3A1  \n",
    "Transporter: e.g. MDR5  \n",
    "Carriers: e.g. ALB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a10ef11c240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDrugBankInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/Drug_Properties/Drugbank_2018-07-03.gml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Full DrugBank Network loaded'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-586>\u001b[0m in \u001b[0;36mread_gml\u001b[0;34m(path, label, destringizer)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/utils/decorators.pyc\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(func_to_be_decorated, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Finally, we call the original function, making sure to close the fobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_to_be_decorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mread_gml\u001b[0;34m(path, label, destringizer)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_gml_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestringizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_gml_lines\u001b[0;34m(lines, label, destringizer)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mdirected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'directed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0munexpected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_kv\u001b[0;34m(curr_token)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# dict start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0munexpected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"an int, float, string or '['\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_dict\u001b[0;34m(curr_token)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'['\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# dict start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"']'\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dict end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_kv\u001b[0;34m(curr_token)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# dict start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0munexpected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"an int, float, string or '['\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_dict\u001b[0;34m(curr_token)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'['\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# dict start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"']'\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dict end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mparse_kv\u001b[0;34m(curr_token)\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 \u001b[0mcurr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# dict start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mcurr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m             '|'.join('(' + pattern + ')' for pattern in patterns))\n\u001b[1;32m    314\u001b[0m         \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/networkx/readwrite/gml.pyc\u001b[0m in \u001b[0;36mfilter_lines\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input is not ASCII-encoded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DrugBankInfo = nx.read_gml('../data/Drug_Properties/Drugbank_2018-07-03.gml')\n",
    "print 'Full DrugBank Network loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "81\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "annotated_enzyme_symbols = set()\n",
    "annotated_transporters_symbols = set()\n",
    "annotated_carriers_symbols = set()\n",
    "\n",
    "\n",
    "for drug in list(DrugBankInfo.nodes()):\n",
    "    \n",
    "    if DrugBankInfo.node[drug].has_key('Enzymes'):\n",
    "        enzymes = [x for x in DrugBankInfo.node[drug]['Enzymes'].strip().split(',') if x != '']\n",
    "        for e in enzymes:\n",
    "            annotated_enzyme_symbols.add(e.split('_')[0])\n",
    "    if DrugBankInfo.node[drug].has_key('Transporters'):\n",
    "        transporters = [x for x in DrugBankInfo.node[drug]['Transporters'].strip().split(',') if x != '']\n",
    "        for t in transporters:\n",
    "            annotated_transporters_symbols.add(t.split('_')[0])\n",
    "        \n",
    "    if DrugBankInfo.node[drug].has_key('Carriers'):\n",
    "        carriers = [x for x in DrugBankInfo.node[drug]['Carriers'].strip().split(',') if x != '']\n",
    "        for c in carriers:\n",
    "            annotated_carriers_symbols.add(c.split('_')[0])\n",
    "\n",
    "\n",
    "print len(annotated_enzyme_symbols)\n",
    "print len(annotated_transporters_symbols)\n",
    "print len(annotated_carriers_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "81\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Parse the enyme, carriers and transporter SYMBOLS to EntrezIDs using mygeneinfo\n",
    "'''\n",
    "\n",
    "import mygene\n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "#Enzymes\n",
    "query = mg.querymany(annotated_enzyme_symbols, scope='symbol', species='human',verbose=False)\n",
    "final_annotated_enzyme_symbols = []\n",
    "final_annotated_enzyme_IDs = []\n",
    "for result in query:\n",
    "    if result.has_key('entrezgene'):\n",
    "        final_annotated_enzyme_symbols.append(result['symbol'])\n",
    "        final_annotated_enzyme_IDs.append(str(result['_id']))\n",
    "\n",
    "#Transporters\n",
    "query = mg.querymany(annotated_transporters_symbols, scope='symbol', species='human',verbose=False)\n",
    "final_annotated_transporters_symbols = []\n",
    "final_annotated_transporters_IDs = []\n",
    "for result in query:\n",
    "    if result.has_key('entrezgene'):\n",
    "        final_annotated_transporters_symbols.append(result['symbol'])\n",
    "        final_annotated_transporters_IDs.append(str(result['_id']))\n",
    "\n",
    "#Carriers\n",
    "query = mg.querymany(annotated_carriers_symbols, scope='symbol', species='human',verbose=False)\n",
    "final_annotated_carriers_symbols = []\n",
    "final_annotated_carriers_IDs = []\n",
    "for result in query:\n",
    "    if result.has_key('entrezgene'):\n",
    "        final_annotated_carriers_symbols.append(result['symbol'])\n",
    "        final_annotated_carriers_IDs.append(str(result['_id']))\n",
    "        \n",
    "\n",
    "print len(final_annotated_enzyme_IDs)\n",
    "print len(final_annotated_transporters_IDs)\n",
    "print len(final_annotated_carriers_IDs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create an output file with the various transporters/enzymes/targets etc. being split\n",
    "'''\n",
    "cloud_DrugBanktargets = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_DrugBank_Targets_ONLY.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_DrugBanktargets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "\n",
    "cloud_targets = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_All_Targets.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_targets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "all_clouds = cloud_targets.keys()\n",
    "all_clouds.sort()\n",
    "\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_TargetsSplit.csv', 'w')\n",
    "fp_out.write('CLOUD,Targets,Transporters,Enzymes,Carriers\\n')\n",
    "\n",
    "targets_number = []\n",
    "enzymes_number = []\n",
    "transporters_number = []\n",
    "carriers_number = []\n",
    "\n",
    "different_targets = set()\n",
    "different_enzymes = set()\n",
    "different_transporters = set()\n",
    "different_carriers = set()\n",
    "\n",
    "all_targets = 0\n",
    "for cloud in all_clouds:\n",
    "    \n",
    "    targets = []\n",
    "    enzymes = []\n",
    "    carriers = []\n",
    "    transporters = []\n",
    "    \n",
    "    for target in cloud_targets[cloud]:\n",
    "        \n",
    "        if target in cloud_DrugBanktargets[cloud]:\n",
    "            targets.append(target)\n",
    "        else:\n",
    "            not_associated = False\n",
    "            if target in final_annotated_enzyme_IDs:\n",
    "                enzymes.append(target)\n",
    "                not_associated = True\n",
    "            if target in final_annotated_transporters_IDs:\n",
    "                transporters.append(target)\n",
    "                not_associated = True\n",
    "            if target in final_annotated_carriers_IDs:\n",
    "                carriers.append(target)\n",
    "                not_associated = True\n",
    "\n",
    "            if not_associated == False:\n",
    "                targets.append(target)\n",
    "    fp_out.write(cloud+','+';'.join(targets)+','+';'.join(transporters)+','+';'.join(enzymes)+','+';'.join(carriers)+'\\n')\n",
    "    \n",
    "    \n",
    "    all_targets += len(targets)\n",
    "    targets_number.append(len(targets))\n",
    "    enzymes_number.append(len(enzymes))\n",
    "    transporters_number.append(len(transporters))\n",
    "    carriers_number.append(len(carriers))\n",
    "    \n",
    "    different_targets = different_targets.union(set(targets))\n",
    "    different_enzymes = different_enzymes.union(set(enzymes))\n",
    "    different_transporters = different_transporters.union(set(transporters))\n",
    "    different_carriers = different_carriers.union(set(carriers))\n",
    "    \n",
    "    \n",
    "    \n",
    "fp_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 10, 3, 6, 37, 2, 3, 5, 3, 15, 13, 20, 2, 5, 12, 16, 4, 1, 5, 4, 1, 42, 73, 30, 3, 3, 2, 12, 2, 16, 12, 3, 0, 10, 2, 33, 5, 1, 6, 4, 6, 11, 44, 20, 11, 15, 11, 27, 6, 16, 19, 24, 33, 2, 19, 12, 25, 51, 13, 4, 78, 68, 68, 3, 13, 4, 41, 22, 6, 271, 24, 23, 4, 7, 10, 36, 5, 11, 21, 2, 19, 58, 54, 2, 16, 20, 34, 28, 3, 19, 40, 1, 2, 4, 4, 6, 4, 3, 2, 46, 15, 0, 2, 8, 15, 2, 1, 46, 11, 1, 50, 13, 67, 33, 19, 8, 1, 9, 0, 13, 61, 1, 4, 9, 14, 3, 3, 7, 4, 1, 4, 28, 15, 0, 10, 1, 2, 15, 1, 32, 17, 29, 7, 16, 1, 8, 9, 6, 4, 2, 21, 1, 4, 22, 1, 2, 8, 9, 16, 2, 1, 4, 2, 16, 9, 1, 21, 26, 4, 10, 2, 14, 32, 4, 7, 1, 16, 6, 8, 3, 3, 0, 22, 1, 31, 9, 4, 8, 1, 7, 17, 36, 4, 12, 6, 1, 12, 7, 18, 4, 1, 1, 13, 99, 5, 2, 1, 12, 1, 4, 2, 3, 6, 9, 1, 4, 13, 135, 1, 17, 4, 7, 5, 5, 16, 5, 1, 13, 4, 3, 6, 3, 123, 32, 1, 13, 3, 5, 2, 13, 4, 24, 13, 3, 5, 6, 2, 19, 4, 24, 1, 1, 5, 9, 7, 2, 2, 0, 12, 5, 18, 1, 2, 1, 1, 7]\n",
      "Mean number of targets: 14.07\n",
      "Median number of targets: 6.00\n",
      "Mean number of enzymes: 4.07\n",
      "Mean number of carriers: 0.20\n",
      "Mean number of transporters: 1.87\n",
      "Total number of targets: 3757\n",
      "Number of distinct targets: 1123\n",
      "Number of distinct  enzymes: 92\n",
      "Number of distinct  carriers: 7\n",
      "Number of distinct  transporters: 62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "\n",
    "print'Mean number of targets: %.2f' %np.mean(targets_number)\n",
    "print'Median number of targets: %.2f' %np.median(targets_number)\n",
    "print'Mean number of enzymes: %.2f' %np.mean(enzymes_number)\n",
    "print'Mean number of carriers: %.2f' %np.mean(carriers_number)\n",
    "print'Mean number of transporters: %.2f' %np.mean(transporters_number)\n",
    "\n",
    "print 'Total number of targets: %d' %all_targets\n",
    "print 'Number of distinct targets: %d' %len(different_targets)\n",
    "print'Number of distinct  enzymes: %d' %len(different_enzymes)\n",
    "print'Number of distinct  carriers: %d' %len(different_carriers)\n",
    "print'Number of distinct  transporters: %d' %len(different_transporters)\n",
    "\n",
    "\n",
    "plt.hist(targets_number,bins=22, color='#40B9D4')\n",
    "plt.axvline(np.mean(targets_number),ls='--', color='grey')\n",
    "plt.savefig('../results/Drug_Properties/CLOUD_TargetsFiltered.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.) Chemical Genetic perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download from http://software.broadinstitute.org/gsea/msigdb/collections.jsp#C5 [December 17. 2018]\n",
    "\n",
    "'''\n",
    "\n",
    "#Get all CLOUD targets\n",
    "cloud_targets = {}\n",
    "fp = open('../data/Drug_Properties/CLOUD_All_Targets.csv', 'r')\n",
    "fp.next()\n",
    "for line in fp:\n",
    "    tmp = line.strip().split(',')\n",
    "    cloud_targets[tmp[0]] = tmp[2].split(';')\n",
    "fp.close()\n",
    "\n",
    "#Find the gene to perturbation associated (one gene can have various associated perturbations)\n",
    "fp = open('../data/Drug_Properties/Msig_ChemGen_Perturbation.gmt','r')\n",
    "gene_to_perturbation = {}\n",
    "for line in fp:\n",
    "    tmp = line.strip().split('\\t')\n",
    "    for gene in tmp[2:]:\n",
    "        if gene_to_perturbation.has_key(gene):\n",
    "            gene_to_perturbation[gene].append(tmp[0])\n",
    "        else:\n",
    "            gene_to_perturbation[gene] = [tmp[0]]\n",
    "fp.close()\n",
    "\n",
    "\n",
    "#find cloud associations via CLOUD --> Targets ===> Perturbations associated with certain targets\n",
    "fp_out = open('../results/Drug_Properties/CLOUD_to_Perturbations.csv', 'w')\n",
    "fp_out.write('CLOUD,Perturbations\\n')\n",
    "for cloud in all_clouds:\n",
    "\n",
    "    perturbations = []\n",
    "    for gene in cloud_targets[cloud]:\n",
    "        if gene_to_perturbation.has_key(gene):\n",
    "            perturbations.extend(gene_to_perturbation[gene])\n",
    "    fp_out.write(cloud+','+';'.join(perturbations)+'\\n')\n",
    "fp_out.close()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
